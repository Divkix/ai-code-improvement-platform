# ---------------------------
# Backend configuration
# ---------------------------

# --- Server ----------------
PORT=8080
HOST=0.0.0.0
GIN_MODE=debug            # debug | release | test

# --- Logging --------------
LOG_LEVEL=info             # debug | info | warn | error (default: info)
LOG_FORMAT=json            # json | text (default: json for production)
LOG_OUTPUT=stdout          # stdout | stderr (default: stdout)

# --- Rate Limiting -------
RATE_LIMIT_ENABLED=true               # Enable/disable rate limiting (default: true)
RATE_LIMIT_REQUESTS_PER_SECOND=10.0   # Requests per second (default: 10.0)
RATE_LIMIT_BURST_SIZE=20              # Burst capacity (default: 20)

# --- MongoDB --------------
MONGODB_URI=mongodb://mongodb:27017/github-analyzer
DB_NAME=github-analyzer

# MongoDB Connection Pooling
MONGODB_MAX_POOL_SIZE=100           # Max connections in pool (default: 100)
MONGODB_MIN_POOL_SIZE=5             # Min connections in pool (default: 5)
MONGODB_MAX_IDLE_TIME=30s           # Max connection idle time (default: 30s)
MONGODB_CONNECT_TIMEOUT=10s         # Connection timeout (default: 10s)
MONGODB_SERVER_SELECTION_TIMEOUT=5s # Server selection timeout (default: 5s)

# --- Auth/JWT -------------
JWT_SECRET=super-secret-development-jwt-key-for-testing-only # REQUIRED

# --- GitHub OAuth & Import ---------
GITHUB_CLIENT_ID=
GITHUB_CLIENT_SECRET=
GITHUB_ENCRYPTION_KEY=          # 16/24/32-byte AES key
GITHUB_BATCH_SIZE=50            # files per batch (default: 50)
GITHUB_MAX_FILE_SIZE=1048576    # max file size in bytes (default: 1MB)

# ---------------------------
# AI & Embeddings
# ---------------------------
# Point BASE_URL to any OpenAI-compatible /v1/embeddings endpoint (OpenAI, Voyage, Groq, LM-Studioâ€¦).
# For local, use: http://host.docker.internal:1234/v1

EMBEDDING_BASE_URL=https://api.openai.com/v1
EMBEDDING_MODEL=voyage-code-3
EMBEDDING_API_KEY=
CHUNK_SIZE=30                        # lines per chunk (default: 30)
CHUNK_OVERLAP_SIZE=10                # overlap lines (default: 10)
EMBEDDING_BATCH_SIZE=50              # chunks per batch (default: 50)


# --- Qdrant ---------------
QDRANT_URL=http://localhost:6334
QDRANT_COLLECTION_NAME=codechunks
VECTOR_DIMENSION=1024                 # Acceptable: 256 | 512 | 768 | 1024 | 2048
ENABLE_QDRANT_REPO_FILTER=true        # true by default
QDRANT_API_KEY=                       # leave empty for local

# ---------------------------
# LLM (chat completion)
# ---------------------------
# For local, use: http://host.docker.internal:1234/v1
LLM_BASE_URL=https://api.openai.com/v1
LLM_MODEL=gpt-4o-mini
LLM_API_KEY=
LLM_REQUEST_TIMEOUT=30s
LLM_CONTEXT_LENGTH=32000              # max context tokens (default: 32000)
MAX_PROMPT_LENGTH=12000               # prompt truncation limit (default: 12000)
CHAT_CONTEXT_CHUNKS=8                 # chunks per chat query (default: 8)
CHAT_VECTOR_WEIGHT=0.7                # vector vs text search weight (default: 0.7)


# ---------------------------
# Frontend configuration
# ---------------------------

# Base URL of the backend (no trailing slash)
VITE_API_URL=http://localhost:8080

# ---- Optional public variables (must start with PUBLIC_) ----
# PUBLIC_APP_VERSION=1.2.0
