# ---------------------------
# Backend configuration
# ---------------------------

# --- Server ----------------
PORT=8080
HOST=0.0.0.0
GIN_MODE=debug            # debug | release | test

# --- MongoDB --------------
MONGODB_URI=mongodb://localhost:27017/github-analyzer
DB_NAME=github-analyzer

# --- Auth/JWT -------------
JWT_SECRET=super-secret-development-jwt-key-for-testing-only # REQUIRED

# --- GitHub OAuth ---------
GITHUB_CLIENT_ID=
GITHUB_CLIENT_SECRET=
GITHUB_ENCRYPTION_KEY=     # 16/24/32-byte AES key

# ---------------------------
# AI & Embeddings
# ---------------------------
# Point BASE_URL to any OpenAI-compatible /v1/embeddings endpoint (OpenAI, Voyage, Groq, LM-Studioâ€¦).
# For local, use: http://host.docker.internal:1234/v1

EMBEDDING_BASE_URL=https://api.openai.com/v1
EMBEDDING_MODEL=voyage-code-3
EMBEDDING_API_KEY=
CHUNK_SIZE=30    # default is 30
CHUNK_OVERLAP_SIZE=10    # default is 10
# --- Qdrant ---------------
QDRANT_URL=http://localhost:6334
QDRANT_COLLECTION_NAME=codechunks
VECTOR_DIMENSION=1024      # Acceptable: 256 | 512 | 768 | 1024 | 2048
ENABLE_QDRANT_REPO_FILTER=true     # true by default
QDRANT_API_KEY=    # leave empty for local

# ---------------------------
# LLM (chat completion)
# ---------------------------
# For local, use: http://host.docker.internal:1234/v1
LLM_BASE_URL=https://api.openai.com/v1
LLM_MODEL=gpt-4o-mini
LLM_API_KEY=
LLM_REQUEST_TIMEOUT=30s
LLM_CONTEXT_LENGTH=32000


# ---------------------------
# Frontend configuration
# ---------------------------

# Base URL of the backend (no trailing slash)
VITE_API_URL=http://localhost:8080

# ---- Optional public variables (must start with PUBLIC_) ----
# PUBLIC_APP_VERSION=1.2.0
